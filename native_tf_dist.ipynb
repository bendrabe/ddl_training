{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any questions or comments should be directed to brabe2@illinois.edu. Data source paths and support are specific to NCSA's HAL system.\n",
    "\n",
    "# TensorFlow native distributed training\n",
    "\n",
    "This tutorial covers distributed training using TensorFlow's native tf.keras API, which allows users to scale their models to multiple GPUs with little code modification. We'll start with a hands-on demonstration of training a model on a single HAL node (4 GPUs) and outline how to set up multi-node training (although it can't be done through jupyter). For this tutorial, we'll do an implementation of SqueezeNet v1.0 trained on ImageNet. The code is fairly standard boilerplate, so you should be able to drop in your own model (defined using tf.keras) and/or dataset (defined using tf.data.Dataset).\n",
    "\n",
    "First of all, for an optimized, well-maintained, and robust implementation of single- and multi-node training, see the official TF ResNet example (https://github.com/tensorflow/models/tree/master/official/vision/image_classification). It is relatively overwhelming to read and parse the critical sections. The goal of this tutorial is to give bare bones boilerplate that you can understand quickly and extend for your own workload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "\n",
    "First, we'll define a model we want to train using tf.keras (https://www.tensorflow.org/guide/keras):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fire_module(inputs, squeeze_depth, expand_depth, weight_decay):\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=squeeze_depth,\n",
    "        kernel_size=[1, 1],\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(weight_decay)) (inputs)\n",
    "    e1x1 = tf.keras.layers.Conv2D(\n",
    "        filters=expand_depth,\n",
    "        kernel_size=[1, 1],\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(weight_decay)) (x)\n",
    "    e3x3 = tf.keras.layers.Conv2D(\n",
    "        filters=expand_depth,\n",
    "        kernel_size=[3, 3],\n",
    "        padding='same',\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling(),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(weight_decay)) (x)\n",
    "    x = tf.keras.layers.Concatenate(1) ([e1x1, e3x3])\n",
    "    return x\n",
    "\n",
    "def squeezenet(input_shape, num_classes, weight_decay):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    if tf.keras.backend.image_data_format() == 'channels_first':\n",
    "        x = tf.keras.layers.Lambda(\n",
    "            lambda x: tf.keras.backend.permute_dimensions(x, (0, 3, 1, 2))\n",
    "        ) (inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=96,\n",
    "        kernel_size=[7, 7],\n",
    "        strides=2,\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.keras.initializers.VarianceScaling,\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(weight_decay)) (x)\n",
    "    x = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3],\n",
    "        strides=2) (x)\n",
    "    x = fire_module(x, 16, 64, weight_decay)\n",
    "    x = fire_module(x, 16, 64, weight_decay)\n",
    "    x = fire_module(x, 32, 128, weight_decay)\n",
    "    x = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3],\n",
    "        strides=2) (x)\n",
    "    x = fire_module(x, 32, 128, weight_decay)\n",
    "    x = fire_module(x, 48, 192, weight_decay)\n",
    "    x = fire_module(x, 48, 192, weight_decay)\n",
    "    x = fire_module(x, 64, 256, weight_decay)\n",
    "    x = tf.keras.layers.MaxPool2D(\n",
    "        pool_size=[3, 3],\n",
    "        strides=2) (x)\n",
    "    x = fire_module(x, 64, 256, weight_decay)\n",
    "    x = tf.keras.layers.Dropout(rate=0.5) (x)\n",
    "    x = tf.keras.layers.Conv2D(\n",
    "        filters=num_classes,\n",
    "        kernel_size=[1, 1],\n",
    "        activation=tf.keras.activations.relu,\n",
    "        kernel_initializer=tf.random_normal_initializer(mean=0.0, stddev=0.01),\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(weight_decay),\n",
    "        bias_regularizer=tf.keras.regularizers.l2(weight_decay)) (x)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D() (x)\n",
    "    outputs = tf.keras.layers.Activation('softmax', dtype='float32')(x)\n",
    "    \n",
    "    return tf.keras.Model(inputs, outputs, name='squeezenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input data pipeline\n",
    "\n",
    "The next (and possibly most critical) step is defining the input data pipeline. Especially in the case of distributed training, a poorly-designed input pipeline can have a massive impact on throughput. Luckily, TensorFlow's tf.data API provides built-ins that can automatically determine a reasonable pipeline configuration. The input pipeline we'll demo was written following the optimization guidelines in this tutorial (https://www.tensorflow.org/guide/data_performance), which is critical for understanding and adapting the following code.\n",
    "\n",
    "The first step is getting your data into a tf.data.Dataset object. We'll first show a simple dataset already stored in an h5 file, then move on to ImageNet which is in TFRecord format. Once you have a tf.data.Dataset object, both are treated the same way. An important thing to keep in mind with a custom dataset is the size of your files. You'll need to aggregate (batch) individual images together into larger files, as a large number of reads on small files will overwhelm the network filesystem, which hurts performance for every job on the system. Note that the SqueezeNet model  as defined above cannot be used on the SVHN dataset, as it has 5 prediction outputs (digits) instead of one. See https://github.com/bendrabe/ddl_training/blob/master/svhn/model_estimator.py for more info. The following reads the SVHN h5 file, generates a dataset object from it, and and prepares it for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SHUFFLE_BUFFER = 10000\n",
    "SVHN_PATH = '/home/shared/svhn/SVHN_multi_digit_norm_grayscale.h5'\n",
    "\n",
    "def get_svhn_inputfn(is_training, data_dir, batch_size):\n",
    "    # read from h5 file, must know or figure out layout of custom dataset\n",
    "    with h5py.File(data_dir,'r') as h5f:\n",
    "        X_train = h5f['X_train'][:]\n",
    "        y_train = h5f['y_train'][:]\n",
    "        X_val = h5f['X_val'][:]\n",
    "        y_val = h5f['y_val'][:]\n",
    "        X_test = h5f['X_test'][:]\n",
    "        y_test = h5f['y_test'][:]\n",
    "    \n",
    "    # training data is shuffled and repeated indefinitely\n",
    "    # shuffle before repeat respects epoch boundaries\n",
    "    # with keras, don't use num_epochs. instead, use model.fit's steps_per_epoch\n",
    "    if is_training:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        dataset = dataset.shuffle(_SHUFFLE_BUFFER).repeat()\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    \n",
    "    # both train/val use same batch size and autotuned prefetching\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ImageNet dataset is stored in TFRecord format for efficiency (https://www.tensorflow.org/tutorials/load_data/tfrecord), it's a bit more complicated to extract. Note that the following depends on how the TFRecords are generated in the first place (feature mapping, data types, file layout), but the data is stored this way on the HAL system in `/home/shared/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_TRAIN_FILES = 1024\n",
    "_DEFAULT_IMAGE_SIZE = 227\n",
    "_NUM_CHANNELS = 3\n",
    "\n",
    "def get_filenames(is_training, data_dir):\n",
    "    \"\"\"Return filenames for dataset.\"\"\"\n",
    "    if is_training:\n",
    "        return [\n",
    "            os.path.join(data_dir, 'train-%05d-of-01024' % i)\n",
    "            for i in range(_NUM_TRAIN_FILES)]\n",
    "    else:\n",
    "        return [\n",
    "            os.path.join(data_dir, 'validation-%05d-of-00128' % i)\n",
    "            for i in range(128)]\n",
    "\n",
    "def parse_serialized_example(serialized_example):\n",
    "    # Dense features in Example proto.\n",
    "    feature_map = {\n",
    "        'image/encoded': tf.io.FixedLenFeature([], dtype=tf.string,\n",
    "                                            default_value=''),\n",
    "        'image/class/label': tf.io.FixedLenFeature([], dtype=tf.int64,\n",
    "                                                default_value=-1),\n",
    "        'image/class/text': tf.io.FixedLenFeature([], dtype=tf.string,\n",
    "                                               default_value=''),\n",
    "    }\n",
    "    sparse_float32 = tf.io.VarLenFeature(dtype=tf.float32)\n",
    "    # Sparse features in Example proto.\n",
    "    feature_map.update(\n",
    "    {k: sparse_float32 for k in ['image/object/bbox/xmin',\n",
    "                                 'image/object/bbox/ymin',\n",
    "                                 'image/object/bbox/xmax',\n",
    "                                 'image/object/bbox/ymax']})\n",
    "\n",
    "    features = tf.io.parse_single_example(serialized_example, feature_map)\n",
    "    label = tf.cast(features['image/class/label'], dtype=tf.int32)\n",
    "\n",
    "    xmin = tf.expand_dims(features['image/object/bbox/xmin'].values, 0)\n",
    "    ymin = tf.expand_dims(features['image/object/bbox/ymin'].values, 0)\n",
    "    xmax = tf.expand_dims(features['image/object/bbox/xmax'].values, 0)\n",
    "    ymax = tf.expand_dims(features['image/object/bbox/ymax'].values, 0)\n",
    "\n",
    "    # Note that we impose an ordering of (y, x) just to make life difficult.\n",
    "    bbox = tf.concat([ymin, xmin, ymax, xmax], 0)\n",
    "\n",
    "    # Force the variable number of bounding boxes into the shape\n",
    "    # [1, num_boxes, coords].\n",
    "    bbox = tf.expand_dims(bbox, 0)\n",
    "    bbox = tf.transpose(bbox, [0, 2, 1])\n",
    "\n",
    "    return features['image/encoded'], label, bbox\n",
    "\n",
    "# PLACE ANY PREPROCESSING (CROPS, FLIPS, NORMALIZATION, ETC) IN THIS FUNCTION\n",
    "def preprocess_image(raw_image):\n",
    "    image = tf.image.decode_jpeg(raw_image, channels=_NUM_CHANNELS)\n",
    "    image = tf.image.resize(\n",
    "        image, [_DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE]\n",
    "    )\n",
    "    image = tf.cast(image, dtype=tf.float32)\n",
    "    return image\n",
    "\n",
    "def parse_record(raw_record):\n",
    "    raw_image, label, _ = parse_serialized_example(raw_record)\n",
    "    image = preprocess_image(raw_image)\n",
    "    # take care with label range, some code uses 1001 which causes tough to diagnose NaN\n",
    "    label = tf.cast(tf.cast(tf.reshape(label, shape=[1]), dtype=tf.int32) - 1,\n",
    "        dtype=tf.float32)\n",
    "    return image, label\n",
    "\n",
    "def get_imagenet_inputfn(is_training, data_dir, batch_size):\n",
    "    filenames = get_filenames(is_training, data_dir)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "\n",
    "    if is_training:\n",
    "        # shuffle input files\n",
    "        dataset = dataset.shuffle(buffer_size=_NUM_TRAIN_FILES)\n",
    "\n",
    "    # convert to individual records\n",
    "    # 10 files read and deserialized in parallel\n",
    "    dataset = dataset.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=10,\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_training:\n",
    "        # shuffle before repeat respects epoch boundaries\n",
    "        dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n",
    "        # with keras, don't use num_epochs. instead, use model.fit's steps_per_epoch\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    # Parses the raw records into images and labels.\n",
    "    dataset = dataset.map(\n",
    "      lambda value: parse_record(value),\n",
    "      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # ops between final prefetch and get_next call to iterator are sync.\n",
    "    # prefetch again to background preprocessing work.\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Adaptive learning rate\n",
    "\n",
    "TensorFlow's `tf.keras` API has several learning rate decay schedules available via built-in operations (https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules). However, many users may want to experiment with custom LR decay schedules. The `tf.keras.optimizers.schedules.LearningRateSchedule` class can be extended with a user-defined, step-dependent LR function.\n",
    "\n",
    "This is critical in large-batch distributed training, as LR warmup has been shown to decrease instability and improve generalization when scaling up your global batch size. In the following code snippet, we define a custom LR schedule that adds linear warmup to the built-in polynomial LR decay schedule (and plots it in tensorboard for debugging):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolynomialDecayWithWarmup(\n",
    "    tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"Polynomial decay with warmup schedule.\"\"\"\n",
    "\n",
    "    def __init__(self, lr0, power, warmup_steps, decay_steps, name=None):\n",
    "        super(PolynomialDecayWithWarmup, self).__init__()\n",
    "\n",
    "        self.lr0 = lr0\n",
    "        self.power = power\n",
    "        self.post_warmup_lr = lr0 * np.power(1 - warmup_steps/decay_steps, power)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_steps = decay_steps\n",
    "        self.name = name\n",
    "        \n",
    "        # caches the LR op so it doesn't get regenerated every time (mem leak)\n",
    "        self.learning_rate_ops_cache = {}\n",
    "\n",
    "    def __call__(self, step):\n",
    "        \n",
    "        graph = tf.compat.v1.get_default_graph()\n",
    "        if graph not in self.learning_rate_ops_cache:\n",
    "            self.learning_rate_ops_cache[graph] = self._get_lr(step)\n",
    "        return self.learning_rate_ops_cache[graph]\n",
    "    \n",
    "    def _get_lr(self, step):\n",
    "        def warmup_lr(step):\n",
    "            return self.post_warmup_lr * (\n",
    "                tf.cast(step, tf.float32) / tf.cast(self.warmup_steps, tf.float32))\n",
    "        def polynomial_lr(step):\n",
    "            return tf.compat.v1.train.polynomial_decay(\n",
    "                learning_rate=self.lr0,\n",
    "                global_step=step,\n",
    "                decay_steps=self.decay_steps,\n",
    "                end_learning_rate=0.0,\n",
    "                power=self.power\n",
    "            )\n",
    "        lr = tf.cond(step < self.warmup_steps,\n",
    "            lambda: warmup_lr(step),\n",
    "            lambda: polynomial_lr(step))\n",
    "        # plot in TB for debugging purposes\n",
    "        tf.summary.scalar('learning_rate', lr)\n",
    "        return lr\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'lr0': self.lr0,\n",
    "            'power': self.power,\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'decay_steps': self.decay_steps,\n",
    "            'name': self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training driver code\n",
    "\n",
    "Now that we've defined our model and input data pipeline, we'll start on the main driver code. It is fairly self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... get_config() missing 1 required positional argument: 'self'\n",
      "INFO:tensorflow:batch_all_reduce: 52 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 52 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "IMAGENET_PATH = '/home/shared/imagenet/tfrecord'\n",
    "IMAGENET_NUM_CLASSES = 1000\n",
    "IMAGENET_NUM_TRAIN_IMAGES = 1281167\n",
    "IMAGENET_NUM_EVAL_IMAGES = 50000\n",
    "\n",
    "CHECKPTS = False\n",
    "MODEL_DIR = 'native_tf_dist/final'\n",
    "N_GPUS = 4\n",
    "NUM_EPOCHS = 68\n",
    "GLOBAL_BATCH_SIZE = 512\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0002\n",
    "LR0 = 0.04\n",
    "LR_DECAY_POWER = 1.0\n",
    "WARMUP_EPOCHS = 3\n",
    "\n",
    "local_batch_size = GLOBAL_BATCH_SIZE // N_GPUS\n",
    "train_steps = IMAGENET_NUM_TRAIN_IMAGES // GLOBAL_BATCH_SIZE\n",
    "eval_steps = IMAGENET_NUM_EVAL_IMAGES // GLOBAL_BATCH_SIZE\n",
    "decay_steps = NUM_EPOCHS*train_steps\n",
    "warmup_steps = WARMUP_EPOCHS*train_steps\n",
    "\n",
    "imagenet_inshape = (_DEFAULT_IMAGE_SIZE, _DEFAULT_IMAGE_SIZE, _NUM_CHANNELS)\n",
    "\n",
    "# determine at runtime NCHW vs NHWC and set in tf.keras\n",
    "if tf.test.is_built_with_cuda():\n",
    "    data_format = 'channels_first'\n",
    "else:\n",
    "    data_format = 'channels_last'\n",
    "tf.keras.backend.set_image_data_format(data_format)\n",
    "\n",
    "# create train and input datasets\n",
    "train_input_dataset = get_imagenet_inputfn( # get_svhn_inputfn\n",
    "    is_training=True,\n",
    "    data_dir=IMAGENET_PATH, # SVHN_PATH\n",
    "    batch_size=GLOBAL_BATCH_SIZE)\n",
    "eval_input_dataset = get_imagenet_inputfn(\n",
    "    is_training=False,\n",
    "    data_dir=IMAGENET_PATH,\n",
    "    batch_size=GLOBAL_BATCH_SIZE)\n",
    "\n",
    "# our custom LR decay schedule\n",
    "lr_sched = PolynomialDecayWithWarmup(LR0, LR_DECAY_POWER, warmup_steps, decay_steps)\n",
    "# could also use one of the pre-defined ones as per below\n",
    "'''\n",
    "lr_sched = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=LR0,\n",
    "    decay_steps=decay_steps,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0)\n",
    "'''\n",
    "\n",
    "# setting distribute strategy and compiling model inside scope is\n",
    "# essentially all you need to go from one device (GPU) to all on the node\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = squeezenet(imagenet_inshape, IMAGENET_NUM_CLASSES, WEIGHT_DECAY)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.SGD(\n",
    "        learning_rate=lr_sched,\n",
    "        momentum=MOMENTUM)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'],\n",
    "        run_eagerly=False\n",
    "    )\n",
    "\n",
    "callbacks = []\n",
    "# tensorboard callback, can add more metrics with tf.summary\n",
    "callbacks.append(\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=MODEL_DIR,\n",
    "        update_freq=100,\n",
    "        profile_batch=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# checkpointing can be helpful for long-running jobs, below code saves every 5 epochs. see docs for more options.\n",
    "if CHECKPTS:\n",
    "    ckpt_full_path = os.path.join(MODEL_DIR, 'model.ckpt-{epoch:04d}')\n",
    "    callbacks.append(tf.keras.callbacks.ModelCheckpoint(ckpt_full_path, save_weights_only=True, save_freq=5))\n",
    "\n",
    "model.fit(\n",
    "    train_input_dataset,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    steps_per_epoch=train_steps,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=eval_input_dataset,\n",
    "    validation_steps=eval_steps, # see github issue 28995\n",
    "    validation_freq=1,\n",
    "    verbose=0 # see github issue 28995, TensorBoard log anyway\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Extension to multi-node training (> 4 gpus on HAL)\n",
    "\n",
    "To re-iterate, with how the jupyter scheduling is configured, you cannot easily perform multi-node training within a jupyter notebook. Here we provide the code modifications as well as a SLURM runscript for running multi-node training as a batch job.\n",
    "\n",
    "An official tutorial can be found at https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras. There are essentially only two necessary changes to the code we've written so far:\n",
    "\n",
    "1) switching the distribute strategy from `tf.distribute.MirroredStrategy` to `tf.distribute.experimental.MultiWorkerMirroredStrategy` and \n",
    "\n",
    "2) setting the `TF_CONFIG` environment variable as per the API spec (https://www.tensorflow.org/guide/distributed_training#TF_CONFIG)\n",
    "\n",
    "As (1) is a one-line change, we only need to discuss (2). When training on multiple nodes, each will run a copy of your script separately. TensorFlow will handle the communication, but you'll need to provide it a `TF_CONFIG` containing a list of worker hostnames as well as the index in that list of the current worker (different for each node). This must be done __before__ initializing your distribute strategy. We can get all this information from SLURM and put it into the JSON serialized format TF expects using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cluster\": {\"worker\": [\"hal09:8888\"]}, \"task\": {\"type\": \"worker\", \"index\": 0}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "\n",
    "PORT_NUMBER=8888\n",
    "\n",
    "# get nodelist from slurm and parse into list\n",
    "slurm_job_nodelist = os.environ[\"SLURM_JOB_NODELIST\"]\n",
    "cmd = \"scontrol show hostname \" + slurm_job_nodelist\n",
    "cmd_result = subprocess.run(cmd.split(), check=True, text=True, stdout=subprocess.PIPE)\n",
    "node_list = cmd_result.stdout.split('\\n')[:-1]\n",
    "\n",
    "# sorting the node list makes each worker index unique / easy to compute\n",
    "node_list.sort()\n",
    "\n",
    "# get the current node's position in list for worker index\n",
    "node = socket.gethostname()\n",
    "node_idx = node_list.index(node)\n",
    "\n",
    "# set env var\n",
    "os.environ[\"TF_CONFIG\"] = json.dumps({\n",
    "    \"cluster\": {\n",
    "        \"worker\": [\"{}:{}\".format(x, PORT_NUMBER) for x in node_list],\n",
    "    },\n",
    "   \"task\": {\"type\": \"worker\", \"index\": node_idx}\n",
    "})\n",
    "\n",
    "# print it for demo purposes\n",
    "print(os.environ[\"TF_CONFIG\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need a SLURM batch script that will allocate the number of nodes we want and run the script once on each node. If you clone the official ResNet example (https://github.com/tensorflow/models/tree/master/official/vision/image_classification) into `~/models_r2.1.0`, a functional batch script would look like the following:\n",
    "\n",
    "    #!/bin/bash\n",
    "\n",
    "    #SBATCH --job-name=\"native-tf-dist\"\n",
    "    #SBATCH --output=\"native-tf-dist.%j.%N.out\"\n",
    "    #SBATCH --error=\"native-tf-dist.%j.%N.err\" \n",
    "    #SBATCH --partition=gpu\n",
    "    #SBATCH --nodes=2\n",
    "    #SBATCH --ntasks-per-node=1\n",
    "    #SBATCH --cpus-per-task=144\n",
    "    #SBATCH --mem-per-cpu=1200\n",
    "    #SBATCH --gres=gpu:v100:4\n",
    "    #SBATCH --export=ALL\n",
    "    #SBATCH -t 24:00:00\n",
    "\n",
    "    source ~/.bashrc\n",
    "    conda activate wmlce-v1.7.0-py3.7\n",
    "    export PYTHONPATH=\"$PYTHONPATH:~/models_r2.1.0/\"\n",
    "\n",
    "    cd models_r2.1.0/official/vision/image_classification/\n",
    "    srun python resnet_imagenet_main.py -bs 1024 -dd /home/shared/imagenet/tfrecord -ds multi_worker_mirrored -md ~/native-tf-dist-bs1024_summary -ng 4 -te 90 -dt fp16 -ara nccl --enable_tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
